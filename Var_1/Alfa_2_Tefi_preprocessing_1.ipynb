{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.max_columns = 100\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://boosters.pro/championship/alfabattle2/data\n",
    "\n",
    "train_transactions_contest - тренировочная выборка с транзакционными данными \n",
    "test_transactions_contest  - тестовая выборка с транзакционными данными \n",
    "app_id - Идентификатор заявки. заявки пронумерованы так, что более поздним заявкам соответствует более поздняя дата\n",
    "amnt - Нормированная сумма транзакции. 0.0 - соответствует пропускам \n",
    "currency - Идентификатор валюты транзакции (1-11)\n",
    "operation_kind - Идентификатор типа транзакции (1-7)\n",
    "card_type - Уникальный идентификатор типа карты (1-175)*\n",
    "operation_type - Идентификатор типа операции по пластиковой карте (1-22)\n",
    "operation_type_group - Идентификатор группы карточных операций, например, дебетовая карта или кредитная карта(1-3(4))\n",
    "ecommerce_flag - Признак электронной коммерции (1-3)\n",
    "payment_system - Идентификатор типа платежной системы (1-7)\n",
    "income_flag - Признак списания/внесения денежных средств на карту (1-3)\n",
    "mcc - Уникальный идентификатор типа торговой точки (1-108)*\n",
    "country - Идентификатор страны транзакции (1-24)\n",
    "city - Идентификатор города транзакции(1-161)*\n",
    "mcc_category - Идентификатор категории магазина транзакции (1-28)\n",
    "day_of_week - День недели, когда транзакция была совершена (7)\n",
    "hour - Час, когда транзакция была совершена (0-23)\n",
    "days_before - Количество дней до даты выдачи кредита (1-359)*\n",
    "weekofyear - Номер недели в году, когда транзакция была совершена (1-53)\n",
    "hour_diff - Количество часов с момента прошлой транзакции для данного клиента (1-8141...)*\n",
    "transaction_number - Порядковый номер транзакции клиента\n",
    "\n",
    "test_target_contest.csv  выборка для построения прогноза\n",
    "train_target.csv  выборка для обучения\n",
    "app_id - Идентификатор заявки. заявки пронумерованы так, что более поздним заявкам соответствует более поздняя дата\n",
    "product - Продукт по которому нужно принять решение, уйдет ли заявитель в дефолт или нет\n",
    "flag - Целевая переменная, 1 - факт ухода в дефолт. Доступна участникам в обучающей выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загрузка данных из файлов в соответствующие переменные\n",
    "\n",
    "path_0 = 'C:/Tefi/Alfa/Data/train_transactions_contest/'\n",
    "path_1 = 'C:/Tefi/Alfa/Data/test_transactions_contest/'\n",
    "path_2 = 'C:/Tefi/Alfa/Data/'\n",
    "\n",
    "#train_data_1 = pd.read_parquet(f'{path_0}part_000_0_to_23646.parquet', engine='pyarrow')\n",
    "#train_data_2 = pd.read_parquet(f'{path_0}part_001_23647_to_47415.parquet', engine='pyarrow')\n",
    "#train_data_3 = pd.read_parquet(f'{path_0}part_002_47416_to_70092.parquet', engine='pyarrow')\n",
    "#train_data_4 = pd.read_parquet(f'{path_0}part_003_70093_to_92989.parquet', engine='pyarrow')\n",
    "#train_data_5 = pd.read_parquet(f'{path_0}part_004_92990_to_115175.parquet', engine='pyarrow')\n",
    "#train_data_6 = pd.read_parquet(f'{path_0}part_005_115176_to_138067.parquet', engine='pyarrow')\n",
    "#train_data_7 = pd.read_parquet(f'{path_0}part_006_138068_to_159724.parquet', engine='pyarrow')\n",
    "#train_data_8 = pd.read_parquet(f'{path_0}part_007_159725_to_180735.parquet', engine='pyarrow')\n",
    "#train_data_9 = pd.read_parquet(f'{path_0}part_008_180736_to_202834.parquet', engine='pyarrow')\n",
    "#train_data_10 = pd.read_parquet(f'{path_0}part_009_202835_to_224283.parquet', engine='pyarrow')\n",
    "#train_data_11 = pd.read_parquet(f'{path_0}part_010_224284_to_245233.parquet', engine='pyarrow')\n",
    "#train_data_12 = pd.read_parquet(f'{path_0}part_011_245234_to_265281.parquet', engine='pyarrow')\n",
    "#train_data_13 = pd.read_parquet(f'{path_0}part_012_265282_to_285632.parquet', engine='pyarrow')\n",
    "#train_data_14 = pd.read_parquet(f'{path_0}part_013_285633_to_306877.parquet', engine='pyarrow')\n",
    "#train_data_15 = pd.read_parquet(f'{path_0}part_014_306878_to_329680.parquet', engine='pyarrow')\n",
    "#train_data_16 = pd.read_parquet(f'{path_0}part_015_329681_to_350977.parquet', engine='pyarrow')\n",
    "#train_data_17 = pd.read_parquet(f'{path_0}part_016_350978_to_372076.parquet', engine='pyarrow')\n",
    "#train_data_18 = pd.read_parquet(f'{path_0}part_017_372077_to_392692.parquet', engine='pyarrow')\n",
    "#train_data_19 = pd.read_parquet(f'{path_0}part_018_392693_to_413981.parquet', engine='pyarrow')\n",
    "#train_data_20 = pd.read_parquet(f'{path_0}part_019_413982_to_434478.parquet', engine='pyarrow')\n",
    "#train_data_21 = pd.read_parquet(f'{path_0}part_020_434479_to_455958.parquet', engine='pyarrow')\n",
    "#train_data_22 = pd.read_parquet(f'{path_0}part_021_455959_to_477221.parquet', engine='pyarrow')\n",
    "#train_data_23 = pd.read_parquet(f'{path_0}part_022_477222_to_496751.parquet', engine='pyarrow')\n",
    "#train_data_24 = pd.read_parquet(f'{path_0}part_023_496752_to_517332.parquet', engine='pyarrow')\n",
    "#train_data_25 = pd.read_parquet(f'{path_0}part_024_517333_to_537036.parquet', engine='pyarrow')\n",
    "##train_data_26 = pd.read_parquet(f'{path_0}part_025_537037_to_557423.parquet', engine='pyarrow')\n",
    "#train_data_27 = pd.read_parquet(f'{path_0}part_026_557424_to_576136.parquet', engine='pyarrow')\n",
    "#train_data_28 = pd.read_parquet(f'{path_0}part_027_576137_to_595745.parquet', engine='pyarrow')\n",
    "#train_data_29 = pd.read_parquet(f'{path_0}part_028_595746_to_615602.parquet', engine='pyarrow')\n",
    "#train_data_30 = pd.read_parquet(f'{path_0}part_029_615603_to_635004.parquet', engine='pyarrow')\n",
    "#train_data_31 = pd.read_parquet(f'{path_0}part_030_635005_to_654605.parquet', engine='pyarrow')\n",
    "#train_data_32 = pd.read_parquet(f'{path_0}part_031_654606_to_673656.parquet', engine='pyarrow')\n",
    "#train_data_33 = pd.read_parquet(f'{path_0}part_032_673657_to_696025.parquet', engine='pyarrow')\n",
    "#train_data_34 = pd.read_parquet(f'{path_0}part_033_696026_to_714545.parquet', engine='pyarrow')\n",
    "#train_data_35 = pd.read_parquet(f'{path_0}part_034_714546_to_733168.parquet', engine='pyarrow')\n",
    "#train_data_36 = pd.read_parquet(f'{path_0}part_035_733169_to_752514.parquet', engine='pyarrow')\n",
    "#train_data_37 = pd.read_parquet(f'{path_0}part_036_752515_to_770940.parquet', engine='pyarrow')\n",
    "#train_data_38 = pd.read_parquet(f'{path_0}part_037_770941_to_788380.parquet', engine='pyarrow')\n",
    "#train_data_39 = pd.read_parquet(f'{path_0}part_038_788381_to_805771.parquet', engine='pyarrow')\n",
    "#train_data_40 = pd.read_parquet(f'{path_0}part_039_805772_to_823299.parquet', engine='pyarrow')\n",
    "#train_data_41 = pd.read_parquet(f'{path_0}part_040_823300_to_841218.parquet', engine='pyarrow')\n",
    "#train_data_42 = pd.read_parquet(f'{path_0}part_041_841219_to_859270.parquet', engine='pyarrow')\n",
    "#train_data_43 = pd.read_parquet(f'{path_0}part_042_859271_to_878521.parquet', engine='pyarrow')\n",
    "#train_data_44 = pd.read_parquet(f'{path_0}part_043_878522_to_896669.parquet', engine='pyarrow')\n",
    "#train_data_45 = pd.read_parquet(f'{path_0}part_044_896670_to_916056.parquet', engine='pyarrow')\n",
    "#train_data_46 = pd.read_parquet(f'{path_0}part_045_916057_to_935131.parquet', engine='pyarrow')\n",
    "#train_data_47 = pd.read_parquet(f'{path_0}part_046_935132_to_951695.parquet', engine='pyarrow')\n",
    "#train_data_48 = pd.read_parquet(f'{path_0}part_047_951696_to_970383.parquet', engine='pyarrow')\n",
    "#train_data_49 = pd.read_parquet(f'{path_0}part_048_970384_to_987313.parquet', engine='pyarrow')\n",
    "#train_data_50 = pd.read_parquet(f'{path_0}part_049_987314_to_1003050.parquet', engine='pyarrow')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_0 = 'C:/Tefi/Alfa/Data/train_transactions_contest/'\n",
    "path_1 = 'C:/Tefi/Alfa/Data/test_transactions_contest/'\n",
    "path_2 = 'C:/Tefi/Alfa/Data/'\n",
    "\n",
    "#test_data_1 = pd.read_parquet(f'{path_1}part_000_1063620_to_1074462.parquet', engine='pyarrow')\n",
    "#test_data_2 = pd.read_parquet(f'{path_1}part_001_1074463_to_1085303.parquet', engine='pyarrow')\n",
    "#test_data_3 = pd.read_parquet(f'{path_1}part_002_1085304_to_1095174.parquet', engine='pyarrow')\n",
    "#test_data_4 = pd.read_parquet(f'{path_1}part_003_1095175_to_1105002.parquet', engine='pyarrow')\n",
    "#test_data_5 = pd.read_parquet(f'{path_1}part_004_1105003_to_1116054.parquet', engine='pyarrow')\n",
    "#test_data_6 = pd.read_parquet(f'{path_1}part_005_1116055_to_1127527.parquet', engine='pyarrow')\n",
    "#test_data_7 = pd.read_parquet(f'{path_1}part_006_1127528_to_1137672.parquet', engine='pyarrow')\n",
    "#test_data_8 = pd.read_parquet(f'{path_1}part_007_1137673_to_1147504.parquet', engine='pyarrow')\n",
    "#test_data_9 = pd.read_parquet(f'{path_1}part_008_1147505_to_1157749.parquet', engine='pyarrow')\n",
    "#test_data_10 = pd.read_parquet(f'{path_1}part_009_1157750_to_1167980.parquet', engine='pyarrow')\n",
    "#test_data_11 = pd.read_parquet(f'{path_1}part_010_1167981_to_1178851.parquet', engine='pyarrow')\n",
    "#test_data_12 = pd.read_parquet(f'{path_1}part_011_1178852_to_1190630.parquet', engine='pyarrow')\n",
    "\n",
    "#test_data_13 = pd.read_parquet(f'{path_1}part_012_1190631_to_1200939.parquet', engine='pyarrow')\n",
    "#test_data_14 = pd.read_parquet(f'{path_1}part_013_1200940_to_1211425.parquet', engine='pyarrow')\n",
    "#test_data_15 = pd.read_parquet(f'{path_1}part_014_1211426_to_1222122.parquet', engine='pyarrow')\n",
    "#test_data_16 = pd.read_parquet(f'{path_1}part_015_1222123_to_1232298.parquet', engine='pyarrow')\n",
    "#test_data_17 = pd.read_parquet(f'{path_1}part_016_1232299_to_1242388.parquet', engine='pyarrow')\n",
    "#test_data_18 = pd.read_parquet(f'{path_1}part_017_1242389_to_1252416.parquet', engine='pyarrow')\n",
    "#test_data_19 = pd.read_parquet(f'{path_1}part_018_1252417_to_1262614.parquet', engine='pyarrow')\n",
    "#test_data_20 = pd.read_parquet(f'{path_1}part_019_1262615_to_1273376.parquet', engine='pyarrow')\n",
    "#test_data_21 = pd.read_parquet(f'{path_1}part_020_1273377_to_1283831.parquet', engine='pyarrow')\n",
    "#test_data_22 = pd.read_parquet(f'{path_1}part_021_1283832_to_1294494.parquet', engine='pyarrow')\n",
    "#test_data_23 = pd.read_parquet(f'{path_1}part_022_1294495_to_1304964.parquet', engine='pyarrow')\n",
    "#test_data_24 = pd.read_parquet(f'{path_1}part_023_1304965_to_1314698.parquet', engine='pyarrow')\n",
    "\n",
    "#test_data_25 = pd.read_parquet(f'{path_1}part_024_1314699_to_1324518.parquet', engine='pyarrow')\n",
    "#test_data_26 = pd.read_parquet(f'{path_1}part_025_1324519_to_1334901.parquet', engine='pyarrow')\n",
    "#test_data_27 = pd.read_parquet(f'{path_1}part_026_1334902_to_1345587.parquet', engine='pyarrow')\n",
    "#test_data_28 = pd.read_parquet(f'{path_1}part_027_1345588_to_1355874.parquet', engine='pyarrow')\n",
    "#test_data_29 = pd.read_parquet(f'{path_1}part_028_1355875_to_1366314.parquet', engine='pyarrow')\n",
    "#test_data_30 = pd.read_parquet(f'{path_1}part_029_1366315_to_1376991.parquet', engine='pyarrow')\n",
    "#test_data_31 = pd.read_parquet(f'{path_1}part_030_1376992_to_1386419.parquet', engine='pyarrow')\n",
    "#test_data_32 = pd.read_parquet(f'{path_1}part_031_1386420_to_1395884.parquet', engine='pyarrow')\n",
    "#test_data_33 = pd.read_parquet(f'{path_1}part_032_1395885_to_1405390.parquet', engine='pyarrow')\n",
    "#test_data_34 = pd.read_parquet(f'{path_1}part_033_1405391_to_1416489.parquet', engine='pyarrow')\n",
    "#test_data_35 = pd.read_parquet(f'{path_1}part_034_1416492_to_1426763.parquet', engine='pyarrow')\n",
    "#test_data_36 = pd.read_parquet(f'{path_1}part_035_1426764_to_1436400.parquet', engine='pyarrow')\n",
    "\n",
    "test_data_37 = pd.read_parquet(f'{path_1}part_036_1436401_to_1448080.parquet', engine='pyarrow')\n",
    "test_data_38 = pd.read_parquet(f'{path_1}part_037_1448081_to_1459730.parquet', engine='pyarrow')\n",
    "test_data_39 = pd.read_parquet(f'{path_1}part_038_1459731_to_1470134.parquet', engine='pyarrow')\n",
    "test_data_40 = pd.read_parquet(f'{path_1}part_039_1470135_to_1479802.parquet', engine='pyarrow')\n",
    "test_data_41 = pd.read_parquet(f'{path_1}part_040_1479803_to_1489232.parquet', engine='pyarrow')\n",
    "test_data_42 = pd.read_parquet(f'{path_1}part_041_1489233_to_1499712.parquet', engine='pyarrow')\n",
    "test_data_43 = pd.read_parquet(f'{path_1}part_042_1499713_to_1510447.parquet', engine='pyarrow')\n",
    "test_data_44 = pd.read_parquet(f'{path_1}part_043_1510448_to_1520793.parquet', engine='pyarrow')\n",
    "test_data_45 = pd.read_parquet(f'{path_1}part_044_1520794_to_1531282.parquet', engine='pyarrow')\n",
    "test_data_46 = pd.read_parquet(f'{path_1}part_045_1531283_to_1541445.parquet', engine='pyarrow')\n",
    "test_data_47 = pd.read_parquet(f'{path_1}part_046_1541446_to_1551040.parquet', engine='pyarrow')\n",
    "test_data_48 = pd.read_parquet(f'{path_1}part_047_1551041_to_1560328.parquet', engine='pyarrow')\n",
    "test_data_49 = pd.read_parquet(f'{path_1}part_048_1560329_to_1570341.parquet', engine='pyarrow')\n",
    "test_data_50 = pd.read_parquet(f'{path_1}part_049_1570342_to_1580442.parquet', engine='pyarrow')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target = pd.read_csv(f'{path_2}alfabattle2_train_target.csv')\n",
    "test_target = pd.read_csv(f'{path_2}alfabattle2_test_target_contest.csv')\n",
    "sample = pd.read_csv(f'{path_2}alfabattle2_alpha_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc_1(name, name_str):\n",
    "    # name - имя переменной train_data_1\n",
    "    \n",
    "    # замена amnt = 0.0 на NaN \n",
    "    name[\"amnt\"] = name[\"amnt\"].replace(0.0, np.nan).copy()\n",
    "    \n",
    "    # кол-во транзакций для каждого пользователя, общая сумма, min, mean, max транзакций для каждого пользователя\n",
    "    train_data_temp = name[['app_id', 'amnt', 'transaction_number']]\n",
    "    train_data_temp['amount_transaction'] = train_data_temp.groupby('app_id')['transaction_number'].transform('count')\n",
    "    train_data_temp['sum_transaction'] = train_data_temp.groupby('app_id')['amnt'].transform('sum')\n",
    "    train_data_temp['max_transaction'] = train_data_temp.groupby('app_id')['amnt'].transform('max')\n",
    "    train_data_temp['mean_transaction'] = train_data_temp.groupby('app_id')['amnt'].transform('mean')\n",
    "    train_data_temp['min_transaction'] = train_data_temp.groupby('app_id')['amnt'].transform('min')\n",
    "\n",
    "    # добавляем в основную переменную amount_transaction и sum_transaction, для расчета других показателей\n",
    "    name['amount_transaction'] = train_data_temp['amount_transaction']\n",
    "    name['sum_transaction'] = train_data_temp['sum_transaction']    \n",
    "    \n",
    "    train_data_temp.drop(['amnt', 'transaction_number'], axis = 1, inplace = True)\n",
    "    train_data_temp.drop_duplicates(keep = 'first', inplace = True)\n",
    "    train_data = train_data_temp.copy()\n",
    "    \n",
    "   \n",
    "    # сумма каждого вида валюты во всех транзакциях\n",
    "    # доля каждого вида валюты во всех транзакциях (по сумме)  \n",
    "    train_data_temp = name[['app_id', 'amnt', 'currency', 'amount_transaction', 'sum_transaction']]\n",
    "    currency = [f'curr_{i}' for i in range(1,12)]\n",
    "    train_data_temp = pd.concat([train_data_temp, pd.get_dummies(train_data_temp['currency'], prefix='curr')], axis=1)\n",
    "    \n",
    "    number = train_data_temp['currency'].value_counts().index.sort_values()\n",
    "    for i in range (1,12):\n",
    "        if i not in number:\n",
    "            train_data_temp[f'curr_{i}'] = 0\n",
    "    \n",
    "    for i in currency:\n",
    "        train_data_temp[f'sum_{i}'] = 0\n",
    "        train_data_temp[f'sum_{i}'][train_data_temp[i] == 1] = train_data_temp.groupby(['app_id', i])['amnt'].transform('sum')\n",
    "        train_data_temp[f'sum_{i}']  = train_data_temp.groupby(['app_id'])[f'sum_{i}'].transform('max')\n",
    "    \n",
    "    for i in currency:\n",
    "        train_data_temp[f'fraction_{i}'] = train_data_temp[f'sum_{i}'] / train_data_temp['sum_transaction']\n",
    "\n",
    "    train_data_temp = train_data_temp.drop(currency, axis = 'columns')\n",
    "    train_data_temp = train_data_temp.drop(['amnt', 'currency', 'amount_transaction', 'sum_transaction'], axis = 'columns')\n",
    "    train_data_temp.drop_duplicates(keep = 'first', inplace = True)\n",
    "    train_data = pd.merge(train_data, train_data_temp, on='app_id', how='inner')\n",
    "    \n",
    "    \n",
    "    # сумма по каждому типу транзакций\n",
    "    # доля каждого типа транзакций (по сумме)\n",
    "    train_data_temp = name[['app_id', 'amnt', 'operation_kind', 'amount_transaction', 'sum_transaction']]\n",
    "    operation_kind = [f'oper_kind_{i}' for i in range(1,8)]\n",
    "    train_data_temp = pd.concat([train_data_temp, pd.get_dummies(train_data_temp['operation_kind'], prefix='oper_kind')], axis=1)\n",
    "\n",
    "    number = train_data_temp['operation_kind'].value_counts().index.sort_values()\n",
    "    for i in range (1,8):\n",
    "        if i not in number:\n",
    "            train_data_temp[f'oper_kind_{i}'] = 0    \n",
    "        \n",
    "    for i in operation_kind:\n",
    "        train_data_temp[f'sum_{i}'] = 0\n",
    "        train_data_temp[f'sum_{i}'][train_data_temp[i] == 1] = train_data_temp.groupby(['app_id', i])['amnt'].transform('sum')\n",
    "        train_data_temp[f'sum_{i}']  = train_data_temp.groupby(['app_id'])[f'sum_{i}'].transform('max')\n",
    " \n",
    "    for i in operation_kind:\n",
    "        train_data_temp[f'fraction_{i}'] = train_data_temp[f'sum_{i}'] / train_data_temp['sum_transaction']\n",
    "\n",
    "    train_data_temp = train_data_temp.drop(operation_kind, axis = 'columns')\n",
    "    train_data_temp = train_data_temp.drop(['amnt', 'operation_kind', 'amount_transaction', 'sum_transaction'], axis = 'columns')\n",
    "    train_data_temp.drop_duplicates(keep = 'first', inplace = True)\n",
    "    train_data = pd.merge(train_data, train_data_temp, on='app_id', how='inner')\n",
    "    \n",
    "    # сумма по типам операции\n",
    "    # доля типов операции (по сумме)\n",
    "    train_data_temp = name[['app_id', 'amnt', 'operation_type', 'amount_transaction', 'sum_transaction']]\n",
    "    operation_type = [f'oper_type_{i}' for i in range(1,23)]\n",
    "    train_data_temp = pd.concat([train_data_temp, pd.get_dummies(train_data_temp['operation_type'], prefix='oper_type')], axis=1)\n",
    "\n",
    "    number = train_data_temp['operation_type'].value_counts().index.sort_values()\n",
    "    for i in range (1,23):\n",
    "        if i not in number:\n",
    "            train_data_temp[f'oper_type_{i}'] = 0\n",
    "        \n",
    "    for i in operation_type:\n",
    "        train_data_temp[f'sum_{i}'] = 0\n",
    "        train_data_temp[f'sum_{i}'][train_data_temp[i] == 1] = train_data_temp.groupby(['app_id', i])['amnt'].transform('sum')\n",
    "        train_data_temp[f'sum_{i}']  = train_data_temp.groupby(['app_id'])[f'sum_{i}'].transform('max')\n",
    "  \n",
    "    for i in operation_type:\n",
    "        train_data_temp[f'fraction_{i}'] = train_data_temp[f'sum_{i}'] / train_data_temp['sum_transaction']\n",
    "    \n",
    "    train_data_temp = train_data_temp.drop(operation_type, axis = 'columns')\n",
    "    train_data_temp = train_data_temp.drop(['amnt', 'operation_type', 'amount_transaction', 'sum_transaction'], axis = 'columns')\n",
    "    train_data_temp.drop_duplicates(keep = 'first', inplace = True)\n",
    "    train_data = pd.merge(train_data, train_data_temp, on='app_id', how='inner')\n",
    "\n",
    "    # сумма по каждой группе типов операций\n",
    "    # доля каждой группы типов операций\n",
    "    train_data_temp = name[['app_id', 'amnt', 'operation_type_group', 'amount_transaction', 'sum_transaction']]\n",
    "    operation_type_group = [f'oper_group_{i}' for i in range(1,5)]\n",
    "    train_data_temp = pd.concat([train_data_temp, pd.get_dummies(train_data_temp['operation_type_group'], prefix='oper_group')], axis=1)\n",
    "\n",
    "    number = train_data_temp['operation_type_group'].value_counts().index.sort_values()\n",
    "    for i in range (1,5):\n",
    "        if i not in number:\n",
    "            train_data_temp[f'oper_group_{i}'] = 0\n",
    "\n",
    "    for i in operation_type_group:\n",
    "        train_data_temp[f'sum_{i}'] = 0\n",
    "        train_data_temp[f'sum_{i}'][train_data_temp[i] == 1] = train_data_temp.groupby(['app_id', i])['amnt'].transform('sum')\n",
    "        train_data_temp[f'sum_{i}']  = train_data_temp.groupby(['app_id'])[f'sum_{i}'].transform('max')\n",
    "    \n",
    "    for i in operation_type_group:\n",
    "        train_data_temp[f'fraction_{i}'] = train_data_temp[f'sum_{i}'] / train_data_temp['sum_transaction']\n",
    "\n",
    "    train_data_temp = train_data_temp.drop(operation_type_group, axis = 'columns')\n",
    "    train_data_temp = train_data_temp.drop(['amnt', 'operation_type_group', 'amount_transaction', 'sum_transaction'], axis = 'columns')\n",
    "    train_data_temp.drop_duplicates(keep = 'first', inplace = True)\n",
    "    train_data = pd.merge(train_data, train_data_temp, on='app_id', how='inner')\n",
    "\n",
    "\n",
    "    # сумма по группе признака эл коммерции\n",
    "    # доля по группе признака эл коммерции\n",
    "    train_data_temp = name[['app_id', 'amnt', 'ecommerce_flag', 'amount_transaction', 'sum_transaction']]\n",
    "    ecommerce_flag = [f'ecomm_flag_{i}' for i in range(1,4)]\n",
    "    train_data_temp = pd.concat([train_data_temp, pd.get_dummies(train_data_temp['ecommerce_flag'], prefix='ecomm_flag')], axis=1)\n",
    "\n",
    "    number = train_data_temp['ecommerce_flag'].value_counts().index.sort_values()\n",
    "    for i in range (1,4):\n",
    "        if i not in number:\n",
    "            train_data_temp[f'ecomm_flag_{i}'] = 0    \n",
    "\n",
    "    for i in ecommerce_flag:\n",
    "        train_data_temp[f'sum_{i}'] = 0\n",
    "        train_data_temp[f'sum_{i}'][train_data_temp[i] == 1] = train_data_temp.groupby(['app_id', i])['amnt'].transform('sum')\n",
    "        train_data_temp[f'sum_{i}']  = train_data_temp.groupby(['app_id'])[f'sum_{i}'].transform('max')\n",
    "\n",
    "    for i in ecommerce_flag:\n",
    "        train_data_temp[f'fraction_{i}'] = train_data_temp[f'sum_{i}'] / train_data_temp['sum_transaction']\n",
    "\n",
    "    train_data_temp = train_data_temp.drop(ecommerce_flag, axis = 'columns')\n",
    "    train_data_temp = train_data_temp.drop(['amnt', 'ecommerce_flag', 'amount_transaction', 'sum_transaction'], axis = 'columns')\n",
    "    train_data_temp.drop_duplicates(keep = 'first', inplace = True)\n",
    "    train_data = pd.merge(train_data, train_data_temp, on='app_id', how='inner')\n",
    "\n",
    "    \n",
    "    # сумма по типу платежной системы\n",
    "    # доля по типу платежной системы\n",
    "    train_data_temp = name[['app_id', 'amnt', 'payment_system', 'amount_transaction', 'sum_transaction']]\n",
    "    payment_system = [f'pay_sys_{i}' for i in range(1,8)]\n",
    "    train_data_temp = pd.concat([train_data_temp, pd.get_dummies(train_data_temp['payment_system'], prefix='pay_sys')], axis=1)\n",
    "\n",
    "    number = train_data_temp['payment_system'].value_counts().index.sort_values()\n",
    "    for i in range (1,8):\n",
    "        if i not in number:\n",
    "            train_data_temp[f'pay_sys_{i}'] = 0     \n",
    "    \n",
    "    for i in payment_system:\n",
    "        train_data_temp[f'sum_{i}'] = 0\n",
    "        train_data_temp[f'sum_{i}'][train_data_temp[i] == 1] = train_data_temp.groupby(['app_id', i])['amnt'].transform('sum')\n",
    "        train_data_temp[f'sum_{i}']  = train_data_temp.groupby(['app_id'])[f'sum_{i}'].transform('max')\n",
    "\n",
    "    for i in payment_system:\n",
    "        train_data_temp[f'fraction_{i}'] = train_data_temp[f'sum_{i}'] / train_data_temp['sum_transaction']\n",
    "\n",
    "    train_data_temp = train_data_temp.drop(payment_system, axis = 'columns')\n",
    "    train_data_temp = train_data_temp.drop(['amnt', 'payment_system', 'amount_transaction', 'sum_transaction'], axis = 'columns')\n",
    "    train_data_temp.drop_duplicates(keep = 'first', inplace = True)\n",
    "    train_data = pd.merge(train_data, train_data_temp, on='app_id', how='inner')\n",
    "\n",
    "\n",
    "    # сумма по типу признака списания/внесения денежных средств на карту\n",
    "    # доля по типу признака списания/внесения денежных средств на карту\n",
    "    train_data_temp = name[['app_id', 'amnt', 'income_flag', 'amount_transaction', 'sum_transaction']]\n",
    "    income_flag = [f'inc_flag_{i}' for i in range(1,4)]\n",
    "    train_data_temp = pd.concat([train_data_temp, pd.get_dummies(train_data_temp['income_flag'], prefix='inc_flag')], axis=1)\n",
    "\n",
    "    number = train_data_temp['income_flag'].value_counts().index.sort_values()\n",
    "    for i in range (1,4):\n",
    "        if i not in number:\n",
    "            train_data_temp[f'inc_flag_{i}'] = 0    \n",
    "        \n",
    "    for i in income_flag:\n",
    "        train_data_temp[f'sum_{i}'] = 0\n",
    "        train_data_temp[f'sum_{i}'][train_data_temp[i] == 1] = train_data_temp.groupby(['app_id', i])['amnt'].transform('sum')\n",
    "        train_data_temp[f'sum_{i}']  = train_data_temp.groupby(['app_id'])[f'sum_{i}'].transform('max')\n",
    "\n",
    "    for i in income_flag:\n",
    "        train_data_temp[f'fraction_{i}'] = train_data_temp[f'sum_{i}'] / train_data_temp['sum_transaction']\n",
    "\n",
    "    train_data_temp = train_data_temp.drop(income_flag, axis = 'columns')\n",
    "    train_data_temp = train_data_temp.drop(['amnt', 'income_flag', 'amount_transaction', 'sum_transaction'], axis = 'columns')\n",
    "    train_data_temp.drop_duplicates(keep = 'first', inplace = True)\n",
    "    train_data = pd.merge(train_data, train_data_temp, on='app_id', how='inner')\n",
    "\n",
    "\n",
    "    # сумма по стране транзакции\n",
    "    # доля страны транзакции (по сумме)\n",
    "    train_data_temp = name[['app_id', 'amnt', 'country', 'amount_transaction', 'sum_transaction']]\n",
    "    country = [f'country_{i}' for i in range(1,25)]\n",
    "    train_data_temp = pd.concat([train_data_temp, pd.get_dummies(train_data_temp['country'], prefix='country')], axis=1)\n",
    "\n",
    "    number = train_data_temp['country'].value_counts().index.sort_values()\n",
    "    for i in range (1,25):\n",
    "        if i not in number:\n",
    "            train_data_temp[f'country_{i}'] = 0     \n",
    "        \n",
    "    for i in country:\n",
    "        train_data_temp[f'sum_{i}'] = 0\n",
    "        train_data_temp[f'sum_{i}'][train_data_temp[i] == 1] = train_data_temp.groupby(['app_id', i])['amnt'].transform('sum')\n",
    "        train_data_temp[f'sum_{i}']  = train_data_temp.groupby(['app_id'])[f'sum_{i}'].transform('max')\n",
    "\n",
    "    for i in country:\n",
    "        train_data_temp[f'fraction_{i}'] = train_data_temp[f'sum_{i}'] / train_data_temp['sum_transaction']\n",
    "\n",
    "    train_data_temp = train_data_temp.drop(country, axis = 'columns')\n",
    "    train_data_temp = train_data_temp.drop(['amnt', 'country', 'amount_transaction', 'sum_transaction'], axis = 'columns')\n",
    "    train_data_temp.drop_duplicates(keep = 'first', inplace = True)\n",
    "    train_data = pd.merge(train_data, train_data_temp, on='app_id', how='inner')\n",
    "\n",
    "\n",
    "    # сумма по категории магазина \n",
    "    # доля по категории магазина\n",
    "    train_data_temp = name[['app_id', 'amnt', 'mcc_category', 'amount_transaction', 'sum_transaction']]\n",
    "    mcc_category = [f'mcc_categ_{i}' for i in range(1,29)]\n",
    "    train_data_temp = pd.concat([train_data_temp, pd.get_dummies(train_data_temp['mcc_category'], prefix='mcc_categ')], axis=1)\n",
    "\n",
    "    number = train_data_temp['mcc_category'].value_counts().index.sort_values()\n",
    "    for i in range (1,29):\n",
    "        if i not in number:\n",
    "            train_data_temp[f'mcc_categ_{i}'] = 0     \n",
    "\n",
    "    for i in mcc_category:\n",
    "        train_data_temp[f'sum_{i}'] = 0\n",
    "        train_data_temp[f'sum_{i}'][train_data_temp[i] == 1] = train_data_temp.groupby(['app_id', i])['amnt'].transform('sum')\n",
    "        train_data_temp[f'sum_{i}']  = train_data_temp.groupby(['app_id'])[f'sum_{i}'].transform('max')\n",
    "\n",
    "    for i in mcc_category:\n",
    "        train_data_temp[f'fraction_{i}'] = train_data_temp[f'sum_{i}'] / train_data_temp['sum_transaction']\n",
    "\n",
    "    train_data_temp = train_data_temp.drop(mcc_category, axis = 'columns')\n",
    "    train_data_temp = train_data_temp.drop(['amnt', 'mcc_category', 'amount_transaction', 'sum_transaction'], axis = 'columns')\n",
    "    train_data_temp.drop_duplicates(keep = 'first', inplace = True)\n",
    "    train_data = pd.merge(train_data, train_data_temp, on='app_id', how='inner')\n",
    "\n",
    "\n",
    "    # сумма по дням недели \n",
    "    # доля по дням недели\n",
    "    train_data_temp = name[['app_id', 'amnt', 'day_of_week', 'amount_transaction', 'sum_transaction']]\n",
    "    day_of_week = [f'day_week_{i}' for i in range(1,8)]\n",
    "    train_data_temp = pd.concat([train_data_temp, pd.get_dummies(train_data_temp['day_of_week'], prefix='day_week')], axis=1)\n",
    "\n",
    "    number = train_data_temp['day_of_week'].value_counts().index.sort_values()\n",
    "    for i in range (1,8):\n",
    "        if i not in number:\n",
    "            train_data_temp[f'day_week_{i}'] = 0    \n",
    "        \n",
    "    for i in day_of_week:\n",
    "        train_data_temp[f'sum_{i}'] = 0\n",
    "        train_data_temp[f'sum_{i}'][train_data_temp[i] == 1] = train_data_temp.groupby(['app_id', i])['amnt'].transform('sum')\n",
    "        train_data_temp[f'sum_{i}']  = train_data_temp.groupby(['app_id'])[f'sum_{i}'].transform('max')\n",
    "\n",
    "    for i in day_of_week:\n",
    "        train_data_temp[f'fraction_{i}'] = train_data_temp[f'sum_{i}'] / train_data_temp['sum_transaction']\n",
    "\n",
    "    train_data_temp = train_data_temp.drop(day_of_week, axis = 'columns')\n",
    "    train_data_temp = train_data_temp.drop(['amnt', 'day_of_week', 'amount_transaction', 'sum_transaction'], axis = 'columns')\n",
    "    train_data_temp.drop_duplicates(keep = 'first', inplace = True)\n",
    "    train_data = pd.merge(train_data, train_data_temp, on='app_id', how='inner')\n",
    "\n",
    "\n",
    "    # сумма по часам \n",
    "    # доля по часам\n",
    "    train_data_temp = name[['app_id', 'amnt', 'hour', 'amount_transaction', 'sum_transaction']]\n",
    "    hour = [f'hour_{i}' for i in range(0,24)]\n",
    "    train_data_temp = pd.concat([train_data_temp, pd.get_dummies(train_data_temp['hour'], prefix='hour')], axis=1)\n",
    "\n",
    "    number = train_data_temp['hour'].value_counts().index.sort_values()\n",
    "    for i in range (0,24):\n",
    "        if i not in number:\n",
    "            train_data_temp[f'hour_{i}'] = 0    \n",
    "    \n",
    "    for i in hour:\n",
    "        train_data_temp[f'sum_{i}'] = 0\n",
    "        train_data_temp[f'sum_{i}'][train_data_temp[i] == 1] = train_data_temp.groupby(['app_id', i])['amnt'].transform('sum')\n",
    "        train_data_temp[f'sum_{i}']  = train_data_temp.groupby(['app_id'])[f'sum_{i}'].transform('max')\n",
    "\n",
    "    for i in hour:\n",
    "        train_data_temp[f'fraction_{i}'] = train_data_temp[f'sum_{i}'] / train_data_temp['sum_transaction']\n",
    "\n",
    "    train_data_temp = train_data_temp.drop(hour, axis = 'columns')\n",
    "    train_data_temp = train_data_temp.drop(['amnt', 'hour', 'amount_transaction', 'sum_transaction'], axis = 'columns')\n",
    "    train_data_temp.drop_duplicates(keep = 'first', inplace = True)\n",
    "    train_data = pd.merge(train_data, train_data_temp, on='app_id', how='inner')\n",
    "\n",
    "\n",
    "    # сумма по неделям\n",
    "    # доля по неделям\n",
    "    train_data_temp = name[['app_id', 'amnt', 'weekofyear', 'amount_transaction', 'sum_transaction']]\n",
    "    weekofyear = [f'weekofyear_{i}' for i in range(1,54)]\n",
    "    train_data_temp = pd.concat([train_data_temp, pd.get_dummies(train_data_temp['weekofyear'], prefix='weekofyear')], axis=1)\n",
    "\n",
    "    number = train_data_temp['weekofyear'].value_counts().index.sort_values()\n",
    "    for i in range (1,54):\n",
    "        if i not in number:\n",
    "            train_data_temp[f'weekofyear_{i}'] = 0    \n",
    "    \n",
    "    for i in weekofyear:\n",
    "        train_data_temp[f'sum_{i}'] = 0\n",
    "        train_data_temp[f'sum_{i}'][train_data_temp[i] == 1] = train_data_temp.groupby(['app_id', i])['amnt'].transform('sum')\n",
    "        train_data_temp[f'sum_{i}']  = train_data_temp.groupby(['app_id'])[f'sum_{i}'].transform('max')\n",
    "\n",
    "    for i in weekofyear:\n",
    "        train_data_temp[f'fraction_{i}'] = train_data_temp[f'sum_{i}'] / train_data_temp['sum_transaction']\n",
    "\n",
    "    train_data_temp = train_data_temp.drop(weekofyear, axis = 'columns')\n",
    "    train_data_temp = train_data_temp.drop(['amnt', 'weekofyear', 'amount_transaction', 'sum_transaction'], axis = 'columns')\n",
    "    train_data_temp.drop_duplicates(keep = 'first', inplace = True)\n",
    "    train_data = pd.merge(train_data, train_data_temp, on='app_id', how='inner')\n",
    "\n",
    "    \n",
    "    filename = f'{name_str}_pro.parquet'\n",
    "    train_data.to_parquet(filename, engine='pyarrow')\n",
    "  \n",
    "\n",
    "    return train_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_1 = preproc_1(train_data_1, 'train_data_1')\n",
    "train_data_2 = preproc_1(train_data_2, 'train_data_2')\n",
    "train_data_3 = preproc_1(train_data_3, 'train_data_3')\n",
    "time.sleep(120)\n",
    "train_data_4 = preproc_1(train_data_4, 'train_data_4')\n",
    "train_data_5 = preproc_1(train_data_5, 'train_data_5')\n",
    "train_data_6 = preproc_1(train_data_6, 'train_data_6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_7 = preproc_1(train_data_7, 'train_data_7')\n",
    "train_data_8 = preproc_1(train_data_8, 'train_data_8')\n",
    "train_data_9 = preproc_1(train_data_9, 'train_data_9')\n",
    "time.sleep(120)\n",
    "train_data_10 = preproc_1(train_data_10, 'train_data_10')\n",
    "train_data_11 = preproc_1(train_data_11, 'train_data_11')\n",
    "train_data_12 = preproc_1(train_data_12, 'train_data_12')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data_13' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-b8d7fbe2f4e1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_data_13\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreproc_1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data_13\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'train_data_13'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtrain_data_14\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreproc_1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data_14\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'train_data_14'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtrain_data_15\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreproc_1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data_15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'train_data_15'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m220\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtrain_data_16\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreproc_1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data_16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'train_data_16'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_data_13' is not defined"
     ]
    }
   ],
   "source": [
    "train_data_13 = preproc_1(train_data_13, 'train_data_13')\n",
    "train_data_14 = preproc_1(train_data_14, 'train_data_14')\n",
    "train_data_15 = preproc_1(train_data_15, 'train_data_15')\n",
    "time.sleep(220)\n",
    "train_data_16 = preproc_1(train_data_16, 'train_data_16')\n",
    "train_data_17 = preproc_1(train_data_17, 'train_data_17')\n",
    "train_data_18 = preproc_1(train_data_18, 'train_data_18')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(220)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_19 = preproc_1(train_data_19, 'train_data_19')\n",
    "train_data_20 = preproc_1(train_data_20, 'train_data_20')\n",
    "train_data_21 = preproc_1(train_data_21, 'train_data_21')\n",
    "time.sleep(220)\n",
    "train_data_22 = preproc_1(train_data_22, 'train_data_22')\n",
    "train_data_23 = preproc_1(train_data_23, 'train_data_23')\n",
    "train_data_24 = preproc_1(train_data_24, 'train_data_24')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_25 = preproc_1(train_data_25, 'train_data_25')\n",
    "train_data_26 = preproc_1(train_data_26, 'train_data_26')\n",
    "train_data_27 = preproc_1(train_data_27, 'train_data_27')\n",
    "time.sleep(120)\n",
    "train_data_28 = preproc_1(train_data_28, 'train_data_28')\n",
    "train_data_29 = preproc_1(train_data_29, 'train_data_29')\n",
    "train_data_30 = preproc_1(train_data_30, 'train_data_30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_31 = preproc_1(train_data_31, 'train_data_31')\n",
    "train_data_32 = preproc_1(train_data_32, 'train_data_32')\n",
    "train_data_33 = preproc_1(train_data_33, 'train_data_33')\n",
    "time.sleep(120)\n",
    "train_data_34 = preproc_1(train_data_34, 'train_data_34')\n",
    "train_data_35 = preproc_1(train_data_35, 'train_data_35')\n",
    "train_data_36 = preproc_1(train_data_36, 'train_data_36')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_37 = preproc_1(train_data_37, 'train_data_37')\n",
    "train_data_38 = preproc_1(train_data_38, 'train_data_38')\n",
    "train_data_39 = preproc_1(train_data_39, 'train_data_39')\n",
    "time.sleep(120)\n",
    "train_data_40 = preproc_1(train_data_40, 'train_data_40')\n",
    "train_data_41 = preproc_1(train_data_41, 'train_data_41')\n",
    "train_data_42 = preproc_1(train_data_42, 'train_data_42')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_43 = preproc_1(train_data_43, 'train_data_43')\n",
    "train_data_44 = preproc_1(train_data_44, 'train_data_44')\n",
    "train_data_45 = preproc_1(train_data_45, 'train_data_45')\n",
    "time.sleep(120)\n",
    "train_data_46 = preproc_1(train_data_46, 'train_data_46')\n",
    "train_data_47 = preproc_1(train_data_47, 'train_data_47')\n",
    "train_data_48 = preproc_1(train_data_48, 'train_data_48')\n",
    "time.sleep(120)\n",
    "train_data_49 = preproc_1(train_data_49, 'train_data_49')\n",
    "train_data_50 = preproc_1(train_data_50, 'train_data_50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_1 = preproc_1(test_data_1, 'test_data_1')\n",
    "test_data_2 = preproc_1(test_data_2, 'test_data_2')\n",
    "test_data_3 = preproc_1(test_data_3, 'test_data_3')\n",
    "time.sleep(120)\n",
    "test_data_4 = preproc_1(test_data_4, 'test_data_4')\n",
    "test_data_5 = preproc_1(test_data_5, 'test_data_5')\n",
    "test_data_6 = preproc_1(test_data_6, 'test_data_6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_7 = preproc_1(test_data_7, 'test_data_7')\n",
    "test_data_8 = preproc_1(test_data_8, 'test_data_8')\n",
    "test_data_9 = preproc_1(test_data_9, 'test_data_9')\n",
    "time.sleep(120)\n",
    "test_data_10 = preproc_1(test_data_10, 'test_data_10')\n",
    "test_data_11 = preproc_1(test_data_11, 'test_data_11')\n",
    "test_data_12 = preproc_1(test_data_12, 'test_data_12')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_13 = preproc_1(test_data_13, 'test_data_13')\n",
    "test_data_14 = preproc_1(test_data_14, 'test_data_14')\n",
    "test_data_15 = preproc_1(test_data_15, 'test_data_15')\n",
    "time.sleep(120)\n",
    "test_data_16 = preproc_1(test_data_16, 'test_data_16')\n",
    "test_data_17 = preproc_1(test_data_17, 'test_data_17')\n",
    "test_data_18 = preproc_1(test_data_18, 'test_data_18')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_19 = preproc_1(test_data_19, 'test_data_19')\n",
    "test_data_20 = preproc_1(test_data_20, 'test_data_20')\n",
    "test_data_21 = preproc_1(test_data_21, 'test_data_21')\n",
    "time.sleep(120)\n",
    "test_data_22 = preproc_1(test_data_22, 'test_data_22')\n",
    "test_data_23 = preproc_1(test_data_23, 'test_data_23')\n",
    "test_data_24 = preproc_1(test_data_24, 'test_data_24')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_25 = preproc_1(test_data_25, 'test_data_25')\n",
    "test_data_26 = preproc_1(test_data_26, 'test_data_26')\n",
    "test_data_27 = preproc_1(test_data_27, 'test_data_27')\n",
    "time.sleep(120)\n",
    "test_data_28 = preproc_1(test_data_28, 'test_data_28')\n",
    "test_data_29 = preproc_1(test_data_29, 'test_data_29')\n",
    "test_data_30 = preproc_1(test_data_30, 'test_data_30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_31 = preproc_1(test_data_31, 'test_data_31')\n",
    "test_data_32 = preproc_1(test_data_32, 'test_data_32')\n",
    "test_data_33 = preproc_1(test_data_33, 'test_data_33')\n",
    "time.sleep(120)\n",
    "test_data_34 = preproc_1(test_data_34, 'test_data_34')\n",
    "test_data_35 = preproc_1(test_data_35, 'test_data_35')\n",
    "test_data_36 = preproc_1(test_data_36, 'test_data_36')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_37 = preproc_1(test_data_37, 'test_data_37')\n",
    "test_data_38 = preproc_1(test_data_38, 'test_data_38')\n",
    "test_data_39 = preproc_1(test_data_39, 'test_data_39')\n",
    "time.sleep(120)\n",
    "test_data_40 = preproc_1(test_data_40, 'test_data_40')\n",
    "test_data_41 = preproc_1(test_data_41, 'test_data_41')\n",
    "test_data_42 = preproc_1(test_data_42, 'test_data_42')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_43 = preproc_1(test_data_43, 'test_data_43')\n",
    "test_data_44 = preproc_1(test_data_44, 'test_data_44')\n",
    "test_data_45 = preproc_1(test_data_45, 'test_data_45')\n",
    "time.sleep(120)\n",
    "test_data_46 = preproc_1(test_data_46, 'test_data_46')\n",
    "test_data_47 = preproc_1(test_data_47, 'test_data_47')\n",
    "test_data_48 = preproc_1(test_data_48, 'test_data_48')\n",
    "time.sleep(120)\n",
    "test_data_49 = preproc_1(test_data_49, 'test_data_49')\n",
    "test_data_50 = preproc_1(test_data_50, 'test_data_50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
